# -*- coding: utf-8 -*-
"""Copy of Workshop_CBRE_Datathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kfhJ7mC5812GId8JbHJMg1L7FQ6uz-s2

## CNN Workshop - CBRE Datathon 2024

The exercise in this notebook is intended to help you understand the concept of convolutional neural networks and equip you with a process to create and train CNNs with transfer learning.

**-Arjun Naga Siddappa**
___

### What you might want to checkout before the workshop:

**Prerequisites**:   
- Understanding of a perceptron, activation functions and Dense Neural Network
(https://www.geeksforgeeks.org/neural-networks-a-beginners-guide/)

- Know how to build, train and perform inference using a simple Dense Neural Network using PyTorch
(https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)


**Suggested reads:**   
- PyTorch Custom Dataset and Dataloaders
(https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)

- Transfer Learning
(https://builtin.com/data-science/transfer-learning)
___

### What is Convolution?

![image.png](https://upload.wikimedia.org/wikipedia/commons/0/04/Convolution_arithmetic_-_Padding_strides.gif)
"""

import cv2
import numpy as np

import matplotlib.pyplot as plt

kernel = cv2.imread("./bin/circle.jpeg")
kernel = cv2.cvtColor(kernel, cv2.COLOR_BGR2GRAY)
kernel = cv2.resize(kernel, (160, 160),cv2.INTER_LINEAR)
plt.imshow(kernel,cmap="gray")

image = cv2.imread("./bin/fruits.jpeg")
image = cv2.resize(image, (500,500))
image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
image = image*(-1)+255
_,image = cv2.threshold(image, 200, 255, cv2.THRESH_BINARY)
plt.imshow(image,cmap='gray')

def convolution(image, kernel):
    # Get image dimensions
    image_height, image_width = image.shape

    # Get kernel dimensions
    kernel_height, kernel_width = kernel.shape

    # Initialize result image
    result = np.zeros_like(image)

    # Perform convolution
    for i in range(image_height - kernel_height + 1):
        for j in range(image_width - kernel_width + 1):
            # Extract region of interest (ROI)
            roi = image[i:i+kernel_height, j:j+kernel_width]

            # Apply kernel to ROI
            conv_value = np.sum(roi * kernel)

            # Set result pixel value
            result[i, j] = conv_value

    return result

res = convolution(image, kernel)
plt.imshow(res, cmap='gray')

"""#### Setting up the environment"""

import os
from PIL import Image
import random
import numpy as np

import zipfile

import torch
from torch import nn
from torchvision import transforms
from torchsummary import summary
from torch.utils.data import Dataset

device = "cuda" #you can use "mps" for macs with M1/M2 chips, "cpu" or "cuda" otherwise

from google.colab import drive
drive.mount('/content/drive')

class ConstructionDataset(Dataset):
    def __init__(self, data_path=".", classes=["Undeveloped land", "Ground Broken", "Concrete Pad", "Framing Going up", "Near completion or completed"], transform=None):
        super().__init__()

        images = []

        for stage in classes:
            files = os.listdir(os.path.join(".", data_path, stage))
            files = [os.path.join('.', data_path, stage, _file) for _file in files][:600]
            files = list(zip(files, [stage] * len(files)))

            images += files

        random.shuffle(images)
        self.images = images
        self.classes2idx = {stage: idx for idx, stage in enumerate(classes)}
        self.transform = transform

    def __getitem__(self, i):
        path, stage = self.images[i]

        label = self.classes2idx[stage]

        image = Image.open(path)
        if image.mode == 'CMYK':
            image = image.convert('RGB')
        image = image.resize((100, 100))
        image = np.array(image)
        image = np.moveaxis(image, -1, 0)
        image = torch.FloatTensor(image).to(device)

        if self.transform:
            image = self.transform(image)

        return image, label

    def __len__(self):
        return len(self.images)

# Example usage:
from torchvision import transforms

# Define transformations (if needed)
transform = transforms.Compose([
    transforms.ToTensor(),
    # Add more transformations as needed
])

# Create an instance of ConstructionDataset for training
construction_dataset = ConstructionDataset(data_path="./data/", transform=transform)

# Example dataloader creation (for training)
dataloader = torch.utils.data.DataLoader(construction_dataset, batch_size=32, shuffle=True)

shapeDataset = ShapeDataset()
shapeDataLoader = torch.utils.data.DataLoader(shapeDataset, batch_size=10, shuffle=True)

class ShapeClassifier(nn.Module):
  pass

classifier = ShapeClassifier()

summary(classifier, (1,100,100))

classifier = classifier.to(device)

loss_func = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(classifier.parameters())

epochs = 100

for epoch in range(epochs):
  print("Epoch", epoch)
  count=0
  tot_loss = 0

  classifier.train()

  for images, labels in shapeDataLoader:
    labels = labels.to(device)
    preds = classifier(images)

    loss = loss_func(preds, labels)
    loss.backward()

    tot_loss+=loss

    if count%40==0:
      #print("Mean loss:",tot_loss.mean())
      tot_loss=0

    count+=1


    optimizer.step()
    optimizer.zero_grad()

kernels = classifier.layer1.weight.data.cpu().numpy()
plt.imshow(kernels[0][0],cmap="gray")

"""#### Data Preparation"""

import os
from PIL import Image
import random
import numpy as np

import zipfile

import torch
from torch import nn
from torchvision import transforms
from torchsummary import summary
from torch.utils.data import Dataset

device = "cuda" #you can use "mps" for macs with M1/M2 chips, "cpu" or "cuda" otherwise

zipfile.ZipFile("/content/drive/MyDrive/data.zip").extractall("./")

classes = ["Undeveloped land", "Ground Broken", "Concrete Pad", "Framing Going up", "Near completion or completed"]
classes2idx = {'Undeveloped land':0, 'Ground Broken':1, 'Concrete Pad':2, 'Framing Going up':3, "Near completion or completed":4}



import os
from PIL import Image

class LandscapeDataset(torch.utils.data.Dataset):
    def __init__(self, data_path, transform=None):
        super().__init__()

        images = []

        for i in classes:
            files = os.listdir(os.path.join(".", data_path, i))
            # Filter out files with the .DS_Store extension
            files = [os.path.join('.', data_path, i, _file) for _file in files if not _file.endswith('.DS_Store')]
            files = list(zip(files, [i]*len(files)))

            images += files

        random.shuffle(images)
        self.images = images

        if transform:
            self.transform = transform

    def __getitem__(self, i):
        path, _class = self.images[i]

        label = classes2idx[_class]

        image = Image.open(path)
        if image.mode == 'CMYK':
            image = image.convert('RGB')
        image = image.resize((224, 224))
        image = np.array(image)
        image = np.moveaxis(image, -1, 0)
        image = torch.FloatTensor(image).to(device)
        image = self.transform(image)

        return image, label

    def __len__(self):
        return len(self.images)

landscapeDataset = LandscapeDataset(data_path="./data/train", transform=transforms.Compose([transforms.Resize(224)]))
landscapeDataloader = torch.utils.data.DataLoader(landscapeDataset, batch_size=100, shuffle=True)

landscapeDataset_test= LandscapeDataset(data_path="./data/test", transform=transforms.Compose([transforms.Resize(224)]))
landscapeDataloader_test = torch.utils.data.DataLoader(landscapeDataset_test, batch_size=100, shuffle=True)

# Check train dataset
print("Train Dataset:")
for image_path, _ in landscapeDataset.images:
    print(image_path)

# Check test dataset
print("\nTest Dataset:")
for image_path, _ in landscapeDataset_test.images:
    print(image_path)

"""#### Building the model"""

vgg16 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)
#summary(vgg16, (3,224,224))

vgg16 = vgg16.to(device)

def get_vgg16_layers(vgg16):

  layers = list(vgg16.children())
  layers = {
      "conv_layers" : layers[0],
      "ada_layers" : layers[1],
      "linear_layers" : layers[2][:5]
  }

  return layers

class LandscapeClassifier(nn.Module):

  def __init__(self):

    super().__init__()

    vgg16_layers = get_vgg16_layers(vgg16)
    self.fe_conv_layers = vgg16_layers["conv_layers"]
    self.fe_ada_layers = vgg16_layers["ada_layers"]
    self.fe_linear_layers = vgg16_layers["linear_layers"]

    self.classifier_linear_layers = nn.Sequential(
        nn.Linear(4096, 512),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(512, 64),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(64, 6)
    )


    self.freeze_layers(self.fe_conv_layers)
    self.freeze_layers(self.fe_ada_layers)
    self.freeze_layers(self.fe_linear_layers)


  def freeze_layers(self,group):
    for param in group.parameters():
      param.requires_grad = False



  def forward(self, x):
    x = self.fe_conv_layers(x)
    # print(x.shape)
    x = self.fe_ada_layers(x)
    # print(x.shape)
    x = x.flatten(1)

    x = self.fe_linear_layers(x)
    # print(x.shape)

    x = self.classifier_linear_layers(x)

    return x

model = LandscapeClassifier().to("cpu")
summary(model, (3,224,224), device="cpu")

model = model.to(device)

"""#### Training"""

model.eval()

all_preds = []
all_targets = []

for images, labels in landscapeDataloader_test:
    preds = model(images)

    all_targets+=list(labels.detach().cpu().numpy())

    preds = torch.argmax(preds, 1)

    all_preds+= list(preds.detach().cpu().numpy())

# Check the number of unique classes in your dataset
unique_classes_dataset = set(classes)

# Check the number of unique labels in all_targets and all_preds
unique_labels_targets = set(all_targets)
unique_labels_preds = set(all_preds)

# Compare the counts
if len(unique_classes_dataset) == len(unique_labels_targets) == len(unique_labels_preds):
    print("The number of unique classes matches the number of labels.")
else:
    print("The number of unique classes does not match the number of labels.")

classes = ["Undeveloped land", "Ground Broken", "Concrete Pad", "Framing Going up", "Near completion or completed"]

from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt

# Assuming all_targets and all_preds are your actual target and prediction arrays
unique_labels = np.unique(np.concatenate((all_targets, all_preds)))

confusion_matrix = metrics.confusion_matrix(all_targets, all_preds, labels=unique_labels)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=unique_labels)

cm_display.plot()
plt.show()

loss_func = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters())

epochs = 50

for epoch in range(epochs):
  print("Epoch", epoch)
  count=0
  tot_loss = 0

  model.train()

  for images, labels in landscapeDataloader:
    labels = labels.to(device)
    preds = model(images)

    loss = loss_func(preds, labels)
    loss.backward()

    tot_loss+=loss

    if count%40==0:
      print("Mean loss:", tot_loss.mean().item())
      tot_loss=0

    count+=1


    optimizer.step()
    optimizer.zero_grad()

"""#### Testing:"""

model.eval()

all_preds = []
all_targets = []

for images, labels in landscapeDataloader_test:
    preds = model(images)

    all_targets+=list(labels.detach().cpu().numpy())

    preds = torch.argmax(preds, 1)

    all_preds+= list(preds.detach().cpu().numpy())

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(all_targets, all_preds)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = classes)

import matplotlib.pyplot as plt

cm_display.plot()
plt.xticks(rotation=45)
plt.show()